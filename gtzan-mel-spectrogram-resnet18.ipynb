{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I try to classify the audio files in the famous GTZAN dataset\n",
    "\n",
    "I have seen that many people are converting the given 30 sec audio files into 3 second files to increase the data. However, I don't agree with this approach. This is because while splitting the data into training, test and validation sets, the datapoints from the same song would be present in all these sets. This would obviously result in a high accuracy due to the skewed nature of the data.\n",
    "\n",
    "I have therefore used the original files for classification and not split the data.\n",
    "\n",
    "<b>Approach:</b>\n",
    "\n",
    "I first convert the audio files into Mel Spectrograms. Then, I downloaded these Mel Spectrograms as images locally for further use.\n",
    "\n",
    "These Mel Spectrograms can be now used as the new dataset to classify the genres. Since it is a image classification task, I use a pretrained ResNet18 model for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-19T08:33:47.267879Z",
     "iopub.status.busy": "2022-11-19T08:33:47.267443Z",
     "iopub.status.idle": "2022-11-19T08:33:51.248403Z",
     "shell.execute_reply": "2022-11-19T08:33:51.247154Z",
     "shell.execute_reply.started": "2022-11-19T08:33:47.267794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from time import time # to import time() as a function, rather than just the time module\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-19T08:33:51.251297Z",
     "iopub.status.busy": "2022-11-19T08:33:51.250613Z",
     "iopub.status.idle": "2022-11-19T08:33:51.371742Z",
     "shell.execute_reply": "2022-11-19T08:33:51.370766Z",
     "shell.execute_reply.started": "2022-11-19T08:33:51.251253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "path = \"../input/gtzan-dataset-music-genre-classification/\"\n",
    "\n",
    "wav_audio_dataset_path = path + \"Data/genres_original/\"\n",
    "\n",
    "spectrogram_images_path = \"./mel_spectrogram_imgs/\"\n",
    "\n",
    "learning_rate = 5e-3\n",
    "batch_size = 128 # OBS ignored\n",
    "hop_length = 512\n",
    "n_fft = 2048\n",
    "\n",
    "genre_dict = {\"blues\":0,\"classical\":1,\"country\":2,\"disco\":3,\"hiphop\":4,\"jazz\":5,\"metal\":6,\"pop\":7,\"reggae\":8,\"rock\":9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  mps\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu'\n",
    "\n",
    "# device = 'cpu'\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "# elif torch.mps.is_available():\n",
    "#     device = torch.device(\"mps\")  # Metal on macOS\n",
    "\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Audio Files into Mel Spectograms and Saving the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-19T08:33:51.374233Z",
     "iopub.status.busy": "2022-11-19T08:33:51.373886Z",
     "iopub.status.idle": "2022-11-19T08:38:22.197650Z",
     "shell.execute_reply": "2022-11-19T08:38:22.196607Z",
     "shell.execute_reply.started": "2022-11-19T08:33:51.374205Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(spectrogram_images_path):\n",
    "    print(\"Transforming the Audio Files into Mel Spectrograms:\")\n",
    "\n",
    "    mel_spectogram_data = {}\n",
    "    for genre in genre_dict.keys():\n",
    "        print(\"\\t\",genre)\n",
    "        \n",
    "        mel_spectogram_data[genre] = []\n",
    "\n",
    "        for name in glob.glob(wav_audio_dataset_path + genre + \"/*\"):\n",
    "            \n",
    "            if(name != \"../input/gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav\"):\n",
    "            \n",
    "                data,sampling_rate = librosa.load(name)\n",
    "\n",
    "                mel_spec = librosa.feature.melspectrogram(y = data.ravel(), sr=sampling_rate,hop_length = hop_length)\n",
    "                mel_spec_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "                mel_spectogram_data[genre].append(mel_spec_db)\n",
    "                \n",
    "\n",
    "    print(\"Saving the Mel Spectrogram Images:\")\n",
    "    os.mkdir(spectrogram_images_path)\n",
    "    for genre in genre_dict.keys():\n",
    "        print(\"\\t\",genre)\n",
    "        try:\n",
    "            os.mkdir(spectrogram_images_path + genre)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for i in range(len(mel_spectogram_data[genre])):\n",
    "\n",
    "            fig, ax = plt.subplots(1, figsize=(12,8))\n",
    "\n",
    "            img = librosa.display.specshow(mel_spectogram_data[genre][i], sr = sampling_rate, hop_length = hop_length,cmap = 'cool',ax=ax)\n",
    "\n",
    "            fig.savefig(spectrogram_images_path + genre + \"/\" + genre + \"_\" + str(i) + \".png\")\n",
    "            \n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-19T08:38:22.200942Z",
     "iopub.status.busy": "2022-11-19T08:38:22.200274Z",
     "iopub.status.idle": "2022-11-19T08:38:24.238217Z",
     "shell.execute_reply": "2022-11-19T08:38:24.237069Z",
     "shell.execute_reply.started": "2022-11-19T08:38:22.200899Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect() # to free up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Transform the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import datasets, transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Temporary transform to load data as tensors\n",
    "# temp_transform = transforms.Compose([transforms.ToTensor()])\n",
    "# temp_dataset = datasets.ImageFolder(spectrogram_images_path, transform=temp_transform)\n",
    "# temp_loader = DataLoader(temp_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# # Calculate mean and std\n",
    "# mean = 0.0\n",
    "# std = 0.0\n",
    "# total_images = 0\n",
    "\n",
    "# for images, _ in temp_loader:\n",
    "#     batch_samples = images.size(0)  # Batch size (number of images)\n",
    "#     images = images.view(batch_samples, images.size(1), -1)  # Flatten H and W\n",
    "#     mean += images.mean(2).sum(0)\n",
    "#     std += images.std(2).sum(0)\n",
    "#     total_images += batch_samples\n",
    "\n",
    "# mean /= total_images\n",
    "# std /= total_images\n",
    "\n",
    "# print(f\"Calculated mean: {mean}, Calculated std: {std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean.shape, std.shape\n",
    "\n",
    "# Calculated mean: tensor([0.4716, 0.9263, 0.9964]), tensor([0.4526, 0.1533, 0.0568])\n",
    "# default values: mean=[0.4931, 0.9151, 0.9960], std=[0.4495, 0.1716, 0.0602]\n",
    "# default function: tensor([-0.0471,  0.0633,  0.0071]) tensor([1.0106, 0.9319, 0.9437]) # before\n",
    "# default function: tensor([-0.0545,  0.0830,  0.0078]) tensor([0.9974, 0.7766, 0.4186]) # after new normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Dataset:  999\n",
      "Train size: 699, Val size: 149, Test size: 151\n",
      "Train batch size: 350, Val batch size: 149, Test batch size: 151\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "mean, std = torch.tensor([0.4716, 0.9263, 0.9964]), torch.tensor([0.4526, 0.1533, 0.0568])\n",
    "\n",
    "# Define Transforms\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resizing images for ResNet\n",
    "    # transforms.Resize(224), \n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.4931, 0.9151, 0.9960], std=[0.4495, 0.1716, 0.0602])  # Normalization\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalization\n",
    "\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resizing images for ResNet\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.4931, 0.9151, 0.9960], std=[0.4495, 0.1716, 0.0602])  # Normalization\n",
    "    transforms.Normalize(mean=mean, std=std)  # Normalization\n",
    "])\n",
    "\n",
    "# Load dataset using ImageFolder\n",
    "dataset = datasets.ImageFolder(spectrogram_images_path)\n",
    "print(\"Size of Dataset: \", len(dataset)) # 1000\n",
    "\n",
    "# Split the data into Train and Validation\n",
    "train_split = 0.7\n",
    "val_split = 0.15\n",
    "\n",
    "train_size = int(train_split * len(dataset))  # 70% for training\n",
    "val_size = int(val_split * len(dataset))   # 15% for validation\n",
    "test_size = len(dataset) - train_size - val_size  # Remaining for testing\n",
    "\n",
    "# Stratified split (manual implementation using indices)\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Applying transforms to datasets\n",
    "train_dataset.dataset.transform = train_transforms\n",
    "val_dataset.dataset.transform = test_transforms\n",
    "test_dataset.dataset.transform = test_transforms\n",
    "\n",
    "# Create data loaders\n",
    "\n",
    "# optimized batch sizes\n",
    "train_bs = (train_size+1) // 2  # 50% of training data\n",
    "# train_bs = (train_size+3) // 4 # 25% of training data\n",
    "val_bs = len(val_dataset)\n",
    "test_bs = len(test_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=train_bs, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=val_bs, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_bs, shuffle=False)\n",
    "\n",
    "# Checking dataset split sizes\n",
    "print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}, Test size: {len(test_dataset)}\")\n",
    "print(f\"Train batch size: {train_bs}, Val batch size: {val_bs}, Test batch size: {test_bs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old [bad] mean & std function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-19T08:38:24.285537Z",
     "iopub.status.busy": "2022-11-19T08:38:24.285118Z",
     "iopub.status.idle": "2022-11-19T08:38:25.332422Z",
     "shell.execute_reply": "2022-11-19T08:38:25.331125Z",
     "shell.execute_reply.started": "2022-11-19T08:38:24.285501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def mean_std(loader):\n",
    "    images, lebels = next(iter(loader))\n",
    "    # shape of images = [b,c,w,h]\n",
    "    mean, std = images.mean([0,2,3]), images.std([0,2,3])\n",
    "    return mean, std\n",
    "\n",
    "# mean, std = mean_std(train_dataloader)\n",
    "# print(\"mean and std: \\n\", mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18 - Transfer Learning\n",
    "- Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet18_Weights\n",
    "resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### freeze params, fc-layer, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-19T08:38:25.334802Z",
     "iopub.status.busy": "2022-11-19T08:38:25.334406Z",
     "iopub.status.idle": "2022-11-19T08:38:28.076103Z",
     "shell.execute_reply": "2022-11-19T08:38:28.075099Z",
     "shell.execute_reply.started": "2022-11-19T08:38:25.334763Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "Number of Input Features in the Last Fully Connected Layer:  512\n"
     ]
    }
   ],
   "source": [
    "# Classes\n",
    "classes = train_dataloader.dataset.dataset.classes\n",
    "print(\"Classes: \", classes)\n",
    "\n",
    "# Fix the trainable parameters\n",
    "for parameter in resnet.parameters():\n",
    "    parameter.requires_grad = False\n",
    "    \n",
    "    \n",
    "# Number of Input Features in the Last Fully Connected Layer\n",
    "in_features = resnet.fc.in_features\n",
    "print(\"Number of Input Features in the Last Fully Connected Layer: \", in_features)\n",
    "\n",
    "# Replacing the Last Fully Connected Layer\n",
    "fc = nn.Linear(in_features=in_features, out_features=len(classes))\n",
    "resnet.fc = fc\n",
    "\n",
    "\n",
    "# Updating the Weights and Bias of the last layer\n",
    "params_to_update = []\n",
    "for name, param in resnet.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "# Define the Loss and Optimizer Functions\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=learning_rate) # default lr = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-19T08:38:28.078017Z",
     "iopub.status.busy": "2022-11-19T08:38:28.077654Z",
     "iopub.status.idle": "2022-11-19T08:38:28.091461Z",
     "shell.execute_reply": "2022-11-19T08:38:28.090340Z",
     "shell.execute_reply.started": "2022-11-19T08:38:28.077981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_dataloader, val_dataloader, num_epoch, patience=5):\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    for epoch in tqdm(range(num_epoch)):\n",
    "        start_time = time()\n",
    "\n",
    "        running_loss, val_loss = 0, 0\n",
    "        correct_train, total_train = 0, 0\n",
    "        correct_val, total_val = 0, 0\n",
    "\n",
    "        # Training the model\n",
    "        model.train()\n",
    "        for images, labels in train_dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad() # clear gradients for next train\n",
    "            # Forward pass\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logging Training Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "            correct_train += (output.argmax(1) == labels).sum().item()\n",
    "            total_train += labels.size(0) # accumulated number of imagess from all batches in the epoch\n",
    "            \n",
    "        # Validating the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_dataloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                output = model(images)\n",
    "                val_loss += criterion(output, labels).item()\n",
    "                correct_val += (output.argmax(1) == labels).sum().item()\n",
    "                total_val += labels.size(0)\n",
    "        \n",
    "        train_losses.append(running_loss / len(train_dataloader)) # average loss per batch (i.e. per 50% of training data)\n",
    "        val_losses.append(val_loss / len(val_dataloader)) # average val loss per batch (i.e. per 100% of validation data)\n",
    "\n",
    "        print(f\"Train Loss: {train_losses[-1]:.3f}, Val Loss: {val_losses[-1]:.3f}, \"\n",
    "              f\"Train Acc: {correct_train / total_train * 100:.2f}%, Val Acc: {correct_val / total_val * 100:.2f}%\")\n",
    "\n",
    "        if val_losses[-1] < best_val_loss:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            no_improve_epochs = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            if correct_val / total_val > 0.70:\n",
    "                torch.save(model.state_dict(), f'checkpoint_{correct_val / total_val * 100:.2f}', _use_new_zipfile_serialization=True)\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-19T08:38:28.095013Z",
     "iopub.status.busy": "2022-11-19T08:38:28.093245Z",
     "iopub.status.idle": "2022-11-19T09:38:05.308334Z",
     "shell.execute_reply": "2022-11-19T09:38:05.302884Z",
     "shell.execute_reply.started": "2022-11-19T08:38:28.094984Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:08<07:17,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.622, Val Loss: 2.464, Train Acc: 10.73%, Val Acc: 16.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:17<07:07,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.379, Val Loss: 2.242, Train Acc: 19.60%, Val Acc: 10.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:26<06:57,  8.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.024, Val Loss: 2.373, Train Acc: 29.04%, Val Acc: 12.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:35<06:46,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.864, Val Loss: 2.109, Train Acc: 35.34%, Val Acc: 24.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:44<06:34,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.650, Val Loss: 1.932, Train Acc: 44.35%, Val Acc: 28.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:52<06:22,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.570, Val Loss: 1.868, Train Acc: 46.21%, Val Acc: 34.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 7/50 [01:01<06:14,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.419, Val Loss: 1.866, Train Acc: 56.37%, Val Acc: 34.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [01:10<06:05,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.362, Val Loss: 1.711, Train Acc: 58.08%, Val Acc: 40.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [01:18<05:56,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.270, Val Loss: 1.544, Train Acc: 61.09%, Val Acc: 46.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [01:27<05:46,  8.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.218, Val Loss: 1.433, Train Acc: 61.80%, Val Acc: 55.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [01:36<05:38,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.147, Val Loss: 1.408, Train Acc: 64.23%, Val Acc: 54.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [01:44<05:28,  8.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.102, Val Loss: 1.431, Train Acc: 67.10%, Val Acc: 53.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 13/50 [01:53<05:20,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.066, Val Loss: 1.321, Train Acc: 69.24%, Val Acc: 53.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [02:02<05:12,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.004, Val Loss: 1.246, Train Acc: 70.53%, Val Acc: 59.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [02:10<05:03,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.969, Val Loss: 1.255, Train Acc: 69.38%, Val Acc: 59.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [02:19<04:55,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.936, Val Loss: 1.251, Train Acc: 71.67%, Val Acc: 57.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [02:28<04:48,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.903, Val Loss: 1.213, Train Acc: 73.68%, Val Acc: 60.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [02:37<04:39,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.874, Val Loss: 1.177, Train Acc: 74.54%, Val Acc: 61.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 19/50 [02:45<04:31,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.848, Val Loss: 1.182, Train Acc: 75.39%, Val Acc: 59.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [02:54<04:22,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.828, Val Loss: 1.190, Train Acc: 76.11%, Val Acc: 59.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [03:03<04:15,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.799, Val Loss: 1.171, Train Acc: 77.40%, Val Acc: 59.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [03:12<04:06,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.778, Val Loss: 1.148, Train Acc: 78.25%, Val Acc: 59.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [03:21<03:57,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.757, Val Loss: 1.163, Train Acc: 78.68%, Val Acc: 59.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [03:29<03:48,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.737, Val Loss: 1.155, Train Acc: 79.26%, Val Acc: 57.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [03:38<03:39,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.713, Val Loss: 1.127, Train Acc: 81.26%, Val Acc: 63.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [03:47<03:30,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.699, Val Loss: 1.127, Train Acc: 81.55%, Val Acc: 60.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 27/50 [03:56<03:22,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.679, Val Loss: 1.150, Train Acc: 82.55%, Val Acc: 58.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [04:04<03:12,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.669, Val Loss: 1.134, Train Acc: 82.98%, Val Acc: 58.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [04:13<03:04,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.643, Val Loss: 1.115, Train Acc: 83.26%, Val Acc: 60.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [04:22<02:55,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.633, Val Loss: 1.116, Train Acc: 84.55%, Val Acc: 59.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [04:31<02:46,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.618, Val Loss: 1.129, Train Acc: 84.69%, Val Acc: 57.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [04:39<02:37,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.602, Val Loss: 1.113, Train Acc: 85.41%, Val Acc: 60.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 33/50 [04:48<02:29,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.587, Val Loss: 1.112, Train Acc: 85.98%, Val Acc: 61.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [04:57<02:20,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.575, Val Loss: 1.120, Train Acc: 86.55%, Val Acc: 55.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [05:06<02:11,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.565, Val Loss: 1.109, Train Acc: 87.27%, Val Acc: 58.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [05:15<02:03,  8.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.555, Val Loss: 1.108, Train Acc: 87.41%, Val Acc: 59.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 37/50 [05:24<01:54,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.540, Val Loss: 1.101, Train Acc: 87.41%, Val Acc: 59.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [05:32<01:45,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.532, Val Loss: 1.110, Train Acc: 87.98%, Val Acc: 58.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 39/50 [05:41<01:36,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.513, Val Loss: 1.126, Train Acc: 89.27%, Val Acc: 57.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [05:50<01:27,  8.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.504, Val Loss: 1.105, Train Acc: 89.56%, Val Acc: 58.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [05:59<01:19,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.494, Val Loss: 1.096, Train Acc: 89.99%, Val Acc: 59.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [06:07<01:10,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.482, Val Loss: 1.106, Train Acc: 89.84%, Val Acc: 58.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 43/50 [06:16<01:01,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.475, Val Loss: 1.107, Train Acc: 90.70%, Val Acc: 59.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [06:25<00:52,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.468, Val Loss: 1.108, Train Acc: 90.84%, Val Acc: 57.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [06:34<00:43,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.455, Val Loss: 1.096, Train Acc: 91.27%, Val Acc: 60.40%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [06:42<00:35,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.447, Val Loss: 1.105, Train Acc: 91.27%, Val Acc: 59.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [06:51<00:26,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.442, Val Loss: 1.120, Train Acc: 90.84%, Val Acc: 57.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [07:00<00:17,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.432, Val Loss: 1.102, Train Acc: 92.42%, Val Acc: 58.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [07:09<00:08,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.423, Val Loss: 1.097, Train Acc: 92.27%, Val Acc: 59.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 49/50 [07:17<00:08,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.415, Val Loss: 1.105, Train Acc: 92.27%, Val Acc: 59.06%\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 12s, sys: 1min 34s, total: 7min 46s\n",
      "Wall time: 7min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_epoch = 50 # default 100\n",
    "\n",
    "resnet, train_losses, val_losses = train(\n",
    "    model=resnet,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    num_epoch=num_epoch,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-19T09:38:21.851197Z",
     "iopub.status.busy": "2022-11-19T09:38:21.850805Z",
     "iopub.status.idle": "2022-11-19T09:38:24.988589Z",
     "shell.execute_reply": "2022-11-19T09:38:24.987527Z",
     "shell.execute_reply.started": "2022-11-19T09:38:21.851165Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 62.91%\n",
      "50 epochs, batch size 350: Acc 62.91%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate(model, test_dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total * 100\n",
    "\n",
    "test_accuracy = evaluate(resnet, test_dataloader)\n",
    "print(f'Test accuracy: {test_accuracy:.2f}%')\n",
    "print(f'{num_epoch} epochs, batch size {train_dataloader.batch_size}: Acc {test_accuracy:.4g}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "lr = e-2\n",
    "- 20 epochs, batch size 350: Acc 58.28%\n",
    "- 65/100 epochs, batch size 699: Acc 62.25%\n",
    "- 19/100 epochs, batch size 175: Acc 61.59% 3min\n",
    "\n",
    "Karpathy's learning rate 3e-4\n",
    "- e-4: 100 epochs, batch size 699: Acc 61.59%\n",
    "- 100 epochs, batch size 699: Acc 52.98% (14min 39s) - 3e-4\n",
    "- 49/50 epochs, batch size 350: Acc 62.91% - 5e-3\n",
    "\n",
    "### 8 epochs: val acc\n",
    "\n",
    "128\n",
    "- batch size  32, acc 40.00%, 6min 39s\n",
    "- batch size 128: acc 23.18%, 72.62s\n",
    "- batch size 128: acc 23.18%: 73.8s\n",
    "- batch size 128: acc 49.01%: 92.3s\n",
    "- batch size 128: acc 49.01%: 96.09s\n",
    "\n",
    "256\n",
    "- batch size 256: acc 42.38%: 74.39s\n",
    "- batch size 256: acc 27.15%: 74.66s\n",
    "\n",
    "No normalization:\n",
    "- batch size 128: acc 29.14%: 88.52s\n",
    "-  batch size 128: acc 31.79%: 92.31s (bad norm)\n",
    "\n",
    "Full run (50 epcochs)\n",
    "- deafult Accuracy: 66.6666\n",
    "- 17:46 11/1 val Accuracy 67.67, test acc 69.70%\n",
    "\n",
    "cpu vs mps\n",
    "- 1 epoch cpu: Wall time: 4min 31s CPU times: user 14min 37\n",
    "- 1 epoch mps: Wall time: 16.8 s CPU times: user 11.7 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lessons Learned\n",
    "- Always use mps! 10x speedup\n",
    "- this script requires at least 2 batches with print_every = 2 \n",
    "    - batch_size = dataset=1 // 2 was therefore optimal\n",
    "- very small batch sizes e.g. 499+499=1 gives bad optimization and slower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blues       0.56      0.56      0.56        16\n",
      "   classical       0.75      0.82      0.78        11\n",
      "     country       0.67      0.53      0.59        19\n",
      "       disco       0.63      0.57      0.60        21\n",
      "      hiphop       0.67      0.77      0.71        13\n",
      "        jazz       0.73      0.67      0.70        12\n",
      "       metal       0.67      0.83      0.74        12\n",
      "         pop       0.80      0.47      0.59        17\n",
      "      reggae       0.79      0.73      0.76        15\n",
      "        rock       0.33      0.53      0.41        15\n",
      "\n",
      "    accuracy                           0.63       151\n",
      "   macro avg       0.66      0.65      0.64       151\n",
      "weighted avg       0.66      0.63      0.63       151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Collect all true labels and predictions\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = resnet(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(all_labels, all_predictions, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model from last checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('checkpoint_72.00')\n",
    "# model.to(device) \n",
    "\n",
    "# latest_checkpoint = \n",
    "# state_dict = torch.load(latest_checkpoint)\n",
    "# resnet.load_state_dict(state_dict)\n",
    "# resnet.to(device)\n",
    "\n",
    "# resnet.eval()\n",
    "# print('evaluating checkpoint:', latest_checkpoint)\n",
    "# test_accuracy = evaluate(resnet, test_dataloader)\n",
    "# print(f'Test accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the fully trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('checkpoint_72.00', map_location='mps')\n",
    "# torch.save(model, 'resnet_model2.pth', _use_new_zipfile_serialization=True)\n",
    "\n",
    "# Checkpoint_72.00 is saved with the default serialization method and 'cpu'\n",
    "# loaded_model = torch.load('resnet_model.pth', map_location='cpu', weights_only=True)\n",
    "# test_accuracy = evaluate(loaded_model, test_dataloader)\n",
    "# print(f'Test accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "# from torch.nn import Conv2d\n",
    "# from torch.nn import BatchNorm2d\n",
    "# from torch.nn import ReLU\n",
    "# from torch.nn import MaxPool2d\n",
    "# from torch.nn import Sequential\n",
    "# from torch.nn import AdaptiveAvgPool2d\n",
    "# from torch.nn import Linear\n",
    "\n",
    "# # import the BasicBlock class\n",
    "# from torchvision.models import ResNet\n",
    "# from torchvision.models.resnet import BasicBlock\n",
    "\n",
    "# torch.serialization.add_safe_globals([Sequential])\n",
    "# torch.serialization.add_safe_globals([Linear])\n",
    "# torch.serialization.add_safe_globals([ReLU])\n",
    "# torch.serialization.add_safe_globals([set])\n",
    "# torch.serialization.add_safe_globals([Conv2d])\n",
    "# torch.serialization.add_safe_globals([BatchNorm2d])\n",
    "# torch.serialization.add_safe_globals([MaxPool2d])\n",
    "# torch.serialization.add_safe_globals([AdaptiveAvgPool2d])\n",
    "\n",
    "# torch.serialization.add_safe_globals([ResNet])\n",
    "# torch.serialization.add_safe_globals([BasicBlock])\n",
    "\n",
    "# loaded_model = torch.load('resnet_model.pth', weights_only=True, map_location='mps')\n",
    "\n",
    "# # evaluate the model\n",
    "# test_accuracy = evaluate(loaded_model, test_dataloader)\n",
    "# print(f'Test accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be a good baseline model for this task.\n",
    "\n",
    "I'll be trying to improve the performance by including more preprocessed features which I have detailed in [this notebook](https://www.kaggle.com/code/nippani/preprocessing-audio-data-gtzan).\n",
    "\n",
    "If you liked this notebook, please upvote.\n",
    "\n",
    "Also, do let me know if there are any corrections/additions needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
